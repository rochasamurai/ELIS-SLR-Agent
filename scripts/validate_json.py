#!/usr/bin/env python3 """ ELIS Validation Script (Extended: repo structure + JSON/JSONL syntax)  Purpose ------- 1) Check minimal repository structure (directories and key files). 2) Verify presence of the canonical XLSX file in /docs. 3) Validate syntax of .json and .jsonl files (UTF-8; JSON Lines supported). 4) Generate a Markdown report under validation_reports/ with a timestamped name. 5) Exit 1 only if a [BLOCKER] is found; otherwise 0.  Usage ----- python scripts/validate_json.py python scripts/validate_json.py --skip-json   # skip JSON/JSONL syntax validation """  from __future__ import annotations  import argparse import io import json import sys from datetime import datetime, timezone from pathlib import Path from typing import Iterable, List  # ---------- Repository paths ---------- ROOT = Path(__file__).resolve().parent.parent DOCS_DIR = ROOT / "docs" SCHEMAS_DIR = ROOT / "schemas" DATA_DIR = ROOT / "json_jsonl" REPORTS_DIR = ROOT / "validation_reports" CANON_XLSX = DOCS_DIR / "ELIS_Data_Sheets_2025-08-19_v1.0.xlsx"  # JSON search scope and extensions JSON_SCAN_DIRS: List[Path] = [DATA_DIR, SCHEMAS_DIR, DOCS_DIR] VALID_EXTS = {".json", ".jsonl"}  # ---------- Time helpers ---------- utc_now = lambda: datetime.now(timezone.utc) ts_isoz = lambda: utc_now().strftime("%Y-%m-%dT%H:%M:%SZ")     # e.g. 2025-09-16T15:42:10Z ts_date = lambda: utc_now().strftime("%Y-%m-%d")               # e.g. 2025-09-16 ts_full = lambda: utc_now().strftime("%Y-%m-%d_%H%M%S")        # e.g. 2025-09-16_154210   # ---------- JSON/JSONL helpers ---------- def iter_json_candidates(paths: Iterable[Path]) -> Iterable[Path]:     """Yield files under given paths that look like JSON/JSONL."""     for base in paths:         if not base.exists():             continue         if base.is_file() and base.suffix.lower() in VALID_EXTS:             yield base         elif base.is_dir():             for f in base.rglob("*"):                 if f.is_file() and f.suffix.lower() in VALID_EXTS:                     yield f   def validate_json_file(fp: Path) -> list[str]:     """Return a list of error messages for a single JSON/JSONL file.      - .json  -> must parse as a single valid JSON document.     - .jsonl -> each non-empty line must be valid JSON.     """     errors: list[str] = []     try:         if fp.suffix.lower() == ".jsonl":             with fp.open("r", encoding="utf-8") as f:                 for i, line in enumerate(f, start=1):                     if not line.strip():                         continue                     try:                         json.loads(line)                     except json.JSONDecodeError as e:                         errors.append(f"[BLOCKER] JSONL syntax: {fp.relative_to(ROOT)}: line {i}: {e.msg}")         else:             # Strict UTF-8; ensure deterministic newline handling             with io.open(fp, "r", encoding="utf-8", newline="\n") as f:                 json.load(f)     except json.JSONDecodeError as e:         errors.append(f"[BLOCKER] JSON syntax: {fp.relative_to(ROOT)}: {e.msg}")     except Exception as e:         errors.append(f"[BLOCKER] JSON read error: {fp.relative_to(ROOT)}: {e}")     return errors   # ---------- Structural checks ---------- def scan_structure() -> List[str]:     """     Perform structural checks and return a list of findings.     Prefixes:       - [BLOCKER] â†’ must fail the CI (exit 1)       - [MINOR]   â†’ advisory only     """     findings: List[str] = []      # Required directories     for d in (DOCS_DIR, SCHEMAS_DIR, DATA_DIR, REPORTS_DIR):         if not d.exists():             findings.append(f"[BLOCKER] Missing required directory: {d.relative_to(ROOT)}")      # Required root-level files     if not (ROOT / "README.md").exists():         findings.append("[MINOR] Missing README.md at repository root")     if not (ROOT / "CHANGELOG.md").exists():         findings.append("[MINOR] Missing CHANGELOG.md at repository root")      # Canonical XLSX     if not CANON_XLSX.exists():         findings.append(f"[BLOCKER] Canonical XLSX not found: {CANON_XLSX.relative_to(ROOT)}")      # Obsolete files to be removed     for junk in (DATA_DIR / "desktop.ini", SCHEMAS_DIR / "desktop.ini"):         if junk.exists():             findings.append(f"[MINOR] Obsolete file present: {junk.relative_to(ROOT)}")      return findings   def scan_json_syntax(skip_json: bool) -> List[str]:     """Validate JSON/JSONL syntax across the repository scope."""     if skip_json:         return ["[MINOR] JSON syntax validation skipped by flag --skip-json"]      findings: List[str] = []     candidates = list(iter_json_candidates(JSON_SCAN_DIRS))     if not candidates:         return findings  # no JSON files is not an error      for fp in candidates:         findings.extend(validate_json_file(fp))     return findings   # ---------- Report writer ---------- def write_report(findings: List[str]) -> Path:     """     Write a Markdown validation report with a unique timestamp.     Returns the path to the generated file.     """     REPORTS_DIR.mkdir(parents=True, exist_ok=True)     out = REPORTS_DIR / f"{ts_date()}_{ts_full()}_validation_report.md"      # Header and metadata     lines: List[str] = [         "# ðŸ“‘ ELIS Validation Report\n",         f"**Generated:** {ts_isoz()}  \n",         "**Scope:** Automatic repository validation (structure, canonical docs, JSON syntax)\n\n",         "---\n\n",         "## âœ… Summary\n",     ]      # Overall status     if any("BLOCKER" in f for f in findings):         lines.append("- Status: **Issues detected (BLOCKER present)**\n")     elif findings:         lines.append("- Status: **Findings detected (no blocker)**\n")     else:         lines.append("- Status: **All critical checks passed**\n")     lines.append("\n---\n\n")      # Checks executed     lines += [         "## 1) Checks Executed\n",         "- Directories: `docs/`, `schemas/`, `json_jsonl/`, `validation_reports/`\n",         "- Root files: `README.md`, `CHANGELOG.md`\n",         f"- Canonical XLSX: `{CANON_XLSX.name}` in `/docs`\n",         "- JSON/JSONL syntax across `docs/`, `schemas/`, `json_jsonl/`\n",         "\n---\n\n",     ]      # Findings     lines.append("## 2) Findings\n")     if findings:         for f in findings:             lines.append(f"- {f}\n")     else:         lines.append("- No issues found.\n")     lines += [         "\n---\n\n",         "## 3) Next Steps\n",         "1. Remove obsolete files (e.g., `desktop.ini`).\n",         "2. Keep the canonical XLSX in `/docs` and updated.\n",         "3. Fix any JSON/JSONL syntax blockers listed above.\n",         "4. (Optional) Extend validation against schemas/XLSX mapping.\n",         "\n---\n\n",         "*Aligned with ELIS Protocol v1.41 and Agent Prompt v2.0.*\n",     ]      out.write_text("".join(lines), encoding="utf-8")     print(f"Validation report written to {out}")  # Log for Actions     return out   # ---------- Main ---------- def main(argv: List[str] | None = None) -> int:     """     Execute the checks and generate the report.     Returns 1 if a [BLOCKER] is found, otherwise 0 (pipeline passes with warnings).     """     ap = argparse.ArgumentParser(description="ELIS repository validation.")     ap.add_argument("--skip-json", action="store_true", help="Skip JSON/JSONL syntax validation.")     args = ap.parse_args(argv)      findings = []     findings += scan_structure()     findings += scan_json_syntax(skip_json=args.skip_json)      write_report(findings)     return 1 if any("BLOCKER" in f for f in findings) else 0   if __name__ == "__main__":     sys.exit(main())