# ==============================================================================
# SEARCH CONFIGURATION: Tai & Awasthi (2025) - Agile Government SLR
# ==============================================================================
# 
# Purpose: Replicate the systematic literature review methodology from:
#   "An exploration of agile government in the public sector: A systematic 
#    literature review at macro, meso, and micro levels of analysis"
#   by Kuang-Ting Tai & Pallavi Awasthi (2025)
#   DOI: 10.1016/j.giq.2025.102082
#
# This configuration file contains all parameters needed to execute database
# searches matching the original paper's methodology for benchmark validation.
#
# Created: 2026-01-30
# Status: Active (Benchmark 2 - Phase 1)
# ==============================================================================

# ------------------------------------------------------------------------------
# SEARCH METADATA
# ------------------------------------------------------------------------------
search_id: "tai_awasthi_2025"
project_name: "Agile Government Systematic Review (Benchmark 2)"
paper_reference:
  title: "An exploration of agile government in the public sector"
  authors:
    - "Kuang-Ting Tai"
    - "Pallavi Awasthi"
  journal: "Government Information Quarterly"
  year: 2025
  doi: "10.1016/j.giq.2025.102082"
  research_questions:
    - "RQ1: How has agility been conceptualized within the context of the public sector?"
    - "RQ2: How has agile government been implemented in public sector settings, and what antecedents or challenges have been identified?"
    - "RQ3: What impacts of agile government in the public sector have been reported in the literature?"
  
created_date: "2026-01-30"
last_modified: "2026-01-30"
status: "active"
version: "2.0.0"

# ------------------------------------------------------------------------------
# SEARCH QUERY
# ------------------------------------------------------------------------------
# The original paper used three keyword combinations applied to titles, 
# abstracts, and author-identified keywords across all databases.
# ------------------------------------------------------------------------------

query:
  # Boolean query string (standard format)
  # This will be translated to database-specific syntax by harvest scripts
  boolean_string: |
    ("agile" AND "government") OR 
    ("agile" AND "governance") OR 
    ("agile" AND "public")
  
  # Individual keyword groups for reference
  keyword_groups:
    - group: 1
      terms: ["agile", "government"]
      operator: "AND"
      
    - group: 2
      terms: ["agile", "governance"]
      operator: "AND"
      
    - group: 3
      terms: ["agile", "public"]
      operator: "AND"
  
  # Fields to search (applied where database allows field-specific search)
  search_fields:
    - "title"
    - "abstract"
    - "keywords"  # Author-identified keywords
  
  # Query Coverage and Research Question Alignment
  query_coverage_notes: |
    QUERY DESIGN RATIONALE:
    
    The original paper's query is intentionally broad and simple:
    - Captures "agile" + public sector context (government/governance/public)
    - Does NOT explicitly filter for conceptualization/implementation/impacts
    - These dimensions were identified during SCREENING phase, not search phase
    
    Research Question Coverage:
    - RQ1 (Conceptualization): Captured implicitly; papers defining/conceptualizing agile
    - RQ2 (Implementation): Captured implicitly; papers describing adoption/application  
    - RQ3 (Impacts): Captured implicitly; papers reporting outcomes/effects
    
    SYSTEMATIC REVIEW APPROACH:
    This approach prioritizes SENSITIVITY (finding all relevant studies) over
    PRECISION (filtering irrelevant studies). Filtering happens in screening.
    
    The authors used a two-phase screening process:
    - Phase 1 (Title/Abstract): 1,102 → 564 (relevance to agile + public sector)
    - Phase 2 (Full-text): 564 → 55 (application of detailed inclusion criteria)
    
    BENCHMARK REPLICATION:
    For ELIS benchmark validation, we replicate this exact query to test if our
    database coverage can retrieve the same 55 final studies they identified.
    Any modifications would invalidate the benchmark comparison.
    
    HYPOTHETICAL EXPANDED QUERY (For Reference Only - NOT USED):
    If this were a new SLR (not replication), we might add:
    - Synonyms: "agility", "agile methods", "agile practices"
    - Context: "public administration", "public sector"
    - Focus terms: "conceptualization", "implementation", "impact", "outcome"
    
    However, the original simple query successfully identified 55 relevant studies
    from diverse sources, demonstrating its effectiveness for this research domain.
  
  # Notes on query interpretation
  notes: |
    - Original paper searched title, abstract, and keywords
    - Boolean operators must be translated to database-specific syntax
    - Some databases (e.g., Google Scholar) may require query simplification
    - Query is intentionally broad; specificity applied during screening

# ------------------------------------------------------------------------------
# DATE RANGE
# ------------------------------------------------------------------------------
# Original paper: "all available years up to the date of the last search in May 2023"
# Earliest article found: 2002
# We use a conservative range to match their actual coverage
# ------------------------------------------------------------------------------

date_range:
  start_year: 2002
  end_year: 2023
  notes: |
    - Original paper: searched up to May 2023
    - No explicit lower bound, but earliest article was 2002
    - Some databases may not support year filtering
    - This range captures the modern era of agile government research
    
    NOTE: This differs from ELIS Protocol v2.0 (1990-2026) because
    this is a BENCHMARK replication that must match the original
    paper's methodology exactly.

# ------------------------------------------------------------------------------
# TARGET DATABASES
# ------------------------------------------------------------------------------
# Original paper used: Web of Science, EBSCOhost, ProQuest
# ELIS Phase 1 uses: Available databases that don't require API access
# 
# Database order follows ELIS Protocol 2025 v2.0 Section 3.2.1
# Order: Scopus, WoS, IEEE Xplore, Semantic Scholar, OpenAlex, CrossRef, CORE, Google Scholar
#
# MAX_RESULTS TIERS (Benchmark-focused):
#   - testing: 25 (quick validation)
#   - pilot: 100 (initial testing)
#   - benchmark: 500 (match scale for validation)
#   - production: 1000 (comprehensive benchmark run)
#   - exhaustive: 99999 (complete coverage)
#
# DEFAULT: benchmark (500) - appropriate scale for benchmark validation
# ------------------------------------------------------------------------------

databases:
  # 1. Scopus
  - name: "Scopus"
    enabled: true
    max_results:
      testing: 25
      pilot: 100
      benchmark: 500
      production: 1000
      exhaustive: 99999
    max_results_default: "benchmark"
    query_syntax: "TITLE-ABS-KEY"
    query_wrapper: "TITLE-ABS-KEY({query})"
    priority: "HIGH"
    notes: |
      - Multidisciplinary database with comprehensive coverage
      - High overlap with Web of Science
      - Requires TITLE-ABS-KEY() wrapper for proper field search
      - Institutional access via API key + token
    api_config:
      rate_limit: "2 requests/second"
      timeout: 30
  
  # 2. Web of Science
  - name: "Web of Science"
    enabled: true
    max_results:
      testing: 25
      pilot: 100
      benchmark: 500
      production: 1000
      exhaustive: 99999
    max_results_default: "benchmark"
    query_syntax: "TS"
    query_wrapper: "TS=({query})"
    priority: "CRITICAL"
    notes: |
      - Direct match with original paper (1/3 databases)
      - Original paper retrieved 405 results from WoS
      - High-impact journal indexing platform
      - Uses TS= wrapper for topic search
    api_config:
      rate_limit: "5 requests/second"
      timeout: 30
  
  # 3. IEEE Xplore
  - name: "IEEE Xplore"
    enabled: true
    max_results:
      testing: 25
      pilot: 100
      benchmark: 200
      production: 500
      exhaustive: 99999
    max_results_default: "benchmark"
    query_syntax: "generic"
    query_filter: "querytext={query}"
    priority: "LOW"
    notes: |
      - Technical literature repository
      - Limited relevance to public administration focus
      - Currently returning 0 results (API configuration issue)
      - Max 200 results per request (API limitation)
    api_config:
      rate_limit: "200 calls/day"
      timeout: 30
  
  # 4. Semantic Scholar
  - name: "Semantic Scholar"
    enabled: true
    max_results:
      testing: 25
      pilot: 100
      benchmark: 200
      production: 500
      exhaustive: 99999
    max_results_default: "benchmark"
    query_syntax: "natural_language"
    query_filter: "query={query}"
    priority: "MEDIUM"
    notes: |
      - AI-enhanced bibliographic database (200M+ papers)
      - Good for CS/digital government topics
      - Natural language query processing
      - Currently experiencing rate limiting issues
    api_config:
      rate_limit: "100 requests/5 minutes"
      timeout: 30
  
  # 5. OpenAlex
  - name: "OpenAlex"
    enabled: true
    max_results:
      testing: 25
      pilot: 100
      benchmark: 500
      production: 1000
      exhaustive: 99999
    max_results_default: "benchmark"
    query_syntax: "default.search"
    query_filter: "default.search:{query}"
    priority: "HIGH"
    notes: |
      - Open bibliographic database (250M+ works)
      - Comprehensive open access aggregator
      - Uses default.search to search title, abstract, fulltext
      - Free API, no authentication required
    api_config:
      rate_limit: "10 requests/second (polite pool)"
      timeout: 30
  
  # 6. CrossRef
  - name: "CrossRef"
    enabled: true
    max_results:
      testing: 25
      pilot: 100
      benchmark: 500
      production: 1000
      exhaustive: 99999
    max_results_default: "benchmark"
    query_syntax: "generic"
    query_filter: "query={query}"
    priority: "MEDIUM"
    notes: |
      - DOI registration agency (130M+ records)
      - DOI-based verification
      - Generic search across all metadata fields
      - Free API with polite pool
    api_config:
      rate_limit: "Polite pool recommended"
      timeout: 30
  
  # 7. CORE
  - name: "CORE"
    enabled: true
    max_results:
      testing: 25
      pilot: 100
      benchmark: 500
      production: 1000
      exhaustive: 99999
    max_results_default: "benchmark"
    query_syntax: "generic"
    query_filter: "q={query}"
    priority: "MEDIUM"
    notes: |
      - Open access aggregator (300M+ papers)
      - Searches title, abstract, fulltext
      - Accepts Boolean operators natively
    api_config:
      rate_limit: "Limited by API key tier"
      timeout: 30
  
  # 8. Google Scholar
  - name: "Google Scholar"
    enabled: true
    max_results:
      testing: 25
      pilot: 100
      benchmark: 100
      production: 500
      exhaustive: 5000
    max_results_default: "pilot"
    query_syntax: "simplified"
    query_transformation: "remove_boolean_operators"
    priority: "HIGH"
    notes: |
      - Comprehensive academic search engine
      - Broadest academic coverage
      - Via Apify EasyAPI actor (easyapi/google-scholar-scraper)
      - FREE TIER LIMITATION: Only 10 results regardless of max_results
      - Paid tier: Up to 5000 results ($10 per 1000)
      - Default set to 'pilot' tier due to free tier limitation
      - Query simplification required (remove Boolean operators)
      - No year filtering support
    api_config:
      provider: "Apify"
      actor: "easyapi/google-scholar-scraper"
      rate_limit: "Managed by Apify"
      timeout: 120

# ------------------------------------------------------------------------------
# MAX_RESULTS TIER SYSTEM - BENCHMARK FOCUS
# ------------------------------------------------------------------------------

max_results_tiers:
  explanation: |
    Benchmark validation uses a focused tier system optimized for
    replicating published methodology at appropriate scale.
  
  tier_definitions:
    testing:
      value: 25
      purpose: "Quick syntax validation"
      time: "~3 minutes"
      cost: "~$0.05"
      when_to_use: "Testing query syntax, API connections"
    
    pilot:
      value: 100
      purpose: "Initial benchmark test"
      time: "~10 minutes"
      cost: "~$0.30"
      when_to_use: "First benchmark run, verify matching works"
    
    benchmark:
      value: 500
      purpose: "Standard benchmark validation"
      time: "~20 minutes"
      cost: "~$1.50"
      when_to_use: "Official benchmark runs, Phase 1 validation"
      note: "Default tier - appropriate scale for benchmark"
    
    production:
      value: 1000
      purpose: "Comprehensive benchmark validation"
      time: "~40 minutes"
      cost: "~$3.00"
      when_to_use: "Final validation, publication-ready results"
    
    exhaustive:
      value: 99999
      purpose: "Complete database coverage"
      time: "~60+ minutes"
      cost: "~$5.00"
      when_to_use: "Maximum possible retrieval for comparison"

# ------------------------------------------------------------------------------
# FILTERS
# ------------------------------------------------------------------------------
# Criteria to narrow down search results to relevant studies
# Based on original paper's inclusion/exclusion criteria
# ------------------------------------------------------------------------------

filters:
  # Language filter
  # Original paper: English-language only
  language: 
    - "English"
  
  # Document types
  # Original paper: Peer-reviewed journal articles only
  # (Excluded: conference papers, book chapters, dissertations, grey literature)
  document_types:
    - "article"
  
  # Peer review requirement
  # Original paper: Only peer-reviewed journals included
  peer_reviewed: true
  
  # Subject areas (optional, not specified in original paper)
  subject_areas: []
  
  # Notes on filtering
  notes: |
    - Original paper excluded non-English publications
    - Only peer-reviewed journal articles were included
    - Conference papers, books, dissertations were excluded
    - No restrictions on subject area (interdisciplinary approach)

# ------------------------------------------------------------------------------
# OUTPUT CONFIGURATION
# ------------------------------------------------------------------------------
# Where and how to store search results
# ------------------------------------------------------------------------------

output:
  # Directory for storing results
  directory: "json_jsonl/"
  
  # Primary output file (combined results from all databases)
  primary_file: "ELIS_Appendix_A_Search_rows.json"
  
  # Backup/archive settings
  archive:
    enabled: true
    directory: "results/tai_awasthi_2025/archives/"
    timestamp_format: "%Y%m%d_%H%M%S"
  
  # Output format options
  format:
    type: "json"
    encoding: "utf-8"
    indent: 2
    ensure_ascii: false
  
  # Deduplication settings
  deduplication:
    enabled: true
    method: "title_normalized"
    case_sensitive: false
    
  # Fields to include in output
  fields:
    required:
      - "source"
      - "title"
      - "authors"
      - "year"
      - "doi"
      - "abstract"
      - "url"
    optional:
      - "citation_count"
      - "keywords"
      - "venue"
      - "pdf_url"
      - "raw_metadata"

# ------------------------------------------------------------------------------
# EXECUTION PARAMETERS
# ------------------------------------------------------------------------------
# Runtime configuration for search execution
# ------------------------------------------------------------------------------

execution:
  # Parallel execution
  parallel_databases: false  # Execute databases sequentially for stability
  
  # Retry configuration
  retry:
    max_attempts: 3
    delay_seconds: 30
    exponential_backoff: true
  
  # Timeout settings (tier-based)
  timeout:
    per_database_minutes:
      testing: 3
      pilot: 8
      benchmark: 12
      production: 20
      exhaustive: 30
    total_search_minutes:
      testing: 20
      pilot: 40
      benchmark: 60
      production: 90
      exhaustive: 120
  
  # Logging
  logging:
    level: "INFO"
    file: "logs/tai_awasthi_2025_search.log"
    console_output: true
  
  # Cost tracking
  cost_tracking:
    enabled: true
    max_budget_usd:
      testing: 0.20
      pilot: 0.50
      benchmark: 2.00
      production: 4.00
      exhaustive: 8.00
    warn_threshold_usd:
      testing: 0.10
      pilot: 0.30
      benchmark: 1.50
      production: 3.00
      exhaustive: 6.00

# ------------------------------------------------------------------------------
# VALIDATION RULES
# ------------------------------------------------------------------------------
# Quality checks to ensure search execution meets standards
# ------------------------------------------------------------------------------

validation:
  # Minimum results per database (tier-based)
  minimum_results:
    testing:
      critical_databases: 10
      other_databases: 5
    pilot:
      critical_databases: 30
      other_databases: 10
    benchmark:
      critical_databases: 100
      other_databases: 10
    production:
      critical_databases: 200
      other_databases: 20
    exhaustive:
      critical_databases: 500
      other_databases: 50
  
  # Expected total results
  expected_range:
    testing:
      minimum: 50
      maximum: 150
    pilot:
      minimum: 200
      maximum: 600
    benchmark:
      minimum: 1500
      maximum: 3000
      notes: "Based on Phase 1 runs: 2000-2500 typical"
    production:
      minimum: 2500
      maximum: 5000
    exhaustive:
      minimum: 4000
      maximum: 8000
  
  # Query validation
  query_checks:
    - "Boolean operators present"
    - "No syntax errors"
    - "Field specifications valid"
  
  # Warn if major deviation from expected
  deviation_threshold: 0.30  # 30% deviation triggers warning

# ------------------------------------------------------------------------------
# NOTES AND CHANGELOG
# ------------------------------------------------------------------------------

notes: |
  This configuration represents our best effort to replicate the Tai & Awasthi
  (2025) search methodology using available ELIS databases. Key differences:
  
  1. Database Coverage:
     - Original: WoS + EBSCOhost + ProQuest (1,102 results)
     - ELIS Phase 1: WoS + 7 other databases (~2,000-2,500 results)
  
  2. Search Fields:
     - Both search: title, abstract, keywords
     - Database-specific syntax properly implemented
  
  3. Limitations:
     - Google Scholar: Free tier limited to 10 results
     - IEEE Xplore: Currently returning 0 results (API issue)
     - Semantic Scholar: Experiencing rate limiting
  
  For benchmark validation, we expect 45-65% retrieval rate (25-35 of 55 studies)
  given that we only have direct access to 1 of 3 original databases.

changelog:
  - version: "1.0.0"
    date: "2026-01-30"
    changes:
      - "Initial creation of search configuration"
      - "Documented all 8 ELIS Phase 1 databases"
      - "Added database-specific query syntax"
      - "Configured output and validation rules"
  
  - version: "1.0.1"
    date: "2026-01-30"
    changes:
      - "Reordered databases to match ELIS Protocol 2025 v2.0"
      - "Order: Scopus, WoS, IEEE, Semantic Scholar, OpenAlex, CrossRef, CORE, Google Scholar"
  
  - version: "1.1.0"
    date: "2026-01-30"
    changes:
      - "Added research questions to metadata"
      - "Added comprehensive query_coverage_notes section"
      - "Documented query design rationale and RQ alignment"
      - "Explained sensitivity vs precision approach"
      - "Added note on hypothetical expanded query for reference"
  
  - version: "2.0.0"
    date: "2026-01-30"
    changes:
      - "MAJOR: Implemented three-tier max_results system"
      - "Added 5 benchmark-focused tiers: testing(25), pilot(100), benchmark(500), production(1000), exhaustive(99999)"
      - "Applied tier system to all 8 databases"
      - "Default tier: benchmark (500) for standard validation"
      - "Added tier-specific timeouts and budget limits"
      - "Documented tier use cases optimized for benchmark validation"
      - "Noted date range difference from ELIS Protocol (benchmark replication)"
